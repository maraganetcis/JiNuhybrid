import streamlit as st
import google.generativeai as genai
import requests
from typing import Dict, List
import json
import time
from datetime import datetime

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹° LLM ì‹œìŠ¤í…œ",
    page_icon="ğŸ§ ",
    layout="wide"
)

class StreamlitAISystem:
    def __init__(self):
        self.setup_api_keys()
        self.initialize_session_state()
    
    def setup_api_keys(self):
        """Streamlit secretsì—ì„œ API í‚¤ ì„¤ì • (ì—†ì„ ê²½ìš° ì²˜ë¦¬)"""
        self.gemini_available = False
        self.openrouter_available = False
        self.deepseek_available = False

        try:
            # Google Gemini
            if 'GOOGLE_API_KEY' in st.secrets:
                genai.configure(api_key=st.secrets['GOOGLE_API_KEY'])
                self.gemini_available = True
            
            # OpenRouter (Claude ë“±)
            if 'OPENROUTER_API_KEY' in st.secrets:
                self.openrouter_key = st.secrets['OPENROUTER_API_KEY']
                self.openrouter_available = True
            
            # DeepSeek
            if 'DEEPSEEK_API_KEY' in st.secrets:
                self.deepseek_key = st.secrets['DEEPSEEK_API_KEY']
                self.deepseek_available = True
                
        except FileNotFoundError:
            st.warning("âš ï¸ `.streamlit/secrets.toml` íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    
    def initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        
        if 'conversation_history' not in st.session_state:
            st.session_state.conversation_history = []
            
        if 'model_usage' not in st.session_state:
            st.session_state.model_usage = {}

    def advanced_intent_analysis(self, user_input: str) -> Dict:
        """ê³ ê¸‰ ì˜ë„ ë¶„ì„ ì‹œìŠ¤í…œ"""
        intent_keywords = {
            'complex_reasoning': [
                'ë…¼ë¦¬', 'ì¶”ë¡ ', 'ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'íŒë‹¨', 'ê²°ë¡ ', 'ê°€ì •',
                'ì „ì œ', 'ë…¼ì¦', 'íƒ€ë‹¹ì„±', 'ë¹„íŒì ', 'ì‚¬ê³ ', 'ì´ìœ ', 'ê·¼ê±°',
                'ë³µì¡í•œ', 'ë‚œì´ë„', 'ì‹¬ì¸µ', 'ë‹¤ë‹¨ê³„', 'ì¢…í•©', 'í†µí•©', 'ì² í•™'
            ],
            'technical': ['ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°', 'ì•Œê³ ë¦¬ì¦˜', 'ê°œë°œ', 'ì„¤ê³„', 'íŒŒì´ì¬', 'ìë°”', 'ì—ëŸ¬', 'ë²„ê·¸', 'api', 'json'],
            'creative': ['ì‘ì„±', 'ìƒì„±', 'ë§Œë“¤', 'ê¸€ì“°ê¸°', 'ì‹œ', 'ì´ì•¼ê¸°', 'ì°½ì˜', 'ì†Œì„¤', 'ì•„ì´ë””ì–´'],
            'mathematical': ['ê³„ì‚°', 'ìˆ˜í•™', 'ê³µì‹', 'ë°©ì •ì‹', 'í†µê³„', 'í™•ë¥ ', 'ë¯¸ë¶„', 'ì ë¶„', 'ìˆ˜ì¹˜'],
            'research': ['ì—°êµ¬', 'ë…¼ë¬¸', 'ì°¸ê³ ë¬¸í—Œ', 'í•™ìˆ ', 'ì´ë¡ ', 'ì‹¤í—˜', 'ë°ì´í„°', 'ë™í–¥'],
            'factual': ['ë­ì•¼', 'ë¬´ì—‡', 'ì•Œë ¤ì¤˜', 'ì •ë³´', 'ì‚¬ì‹¤', 'ì •ì˜', 'ì„¤ëª…'],
            'casual': ['ì•ˆë…•', 'í•˜ì´', 'ì˜ì§€ë‚´', 'ê³ ë§ˆì›Œ', 'ë°˜ê°€ì›Œ']
        }
        
        # ì˜ë„ ì ìˆ˜ ê³„ì‚°
        intent_scores = {}
        user_lower = user_input.lower()
        
        for intent, keywords in intent_keywords.items():
            score = sum(10 for keyword in keywords if keyword in user_lower)
            if score > 0:
                intent_scores[intent] = score
        
        # ë³µì¡ë„ ë¶„ì„ ê°•í™”
        word_count = len(user_input.split())
        has_complex_indicators = any(word in user_lower for word in [
            'ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'ë…¼ë¦¬', 'ì¶”ë¡ ', 'ì „ì œ', 'ê²°ë¡ '
        ])
        
        if word_count > 30 or has_complex_indicators:
            complexity = 'very_high'
        elif word_count > 15:
            complexity = 'high'
        elif word_count > 7:
            complexity = 'medium'
        else:
            complexity = 'low'
        
        # ì£¼ìš” ì˜ë„ ì„ íƒ
        primary_intent = 'general'
        if intent_scores:
            # ë³µì¡í•œ ì¶”ë¡  í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            if 'complex_reasoning' in intent_scores and intent_scores['complex_reasoning'] >= 10:
                primary_intent = 'complex_reasoning'
            else:
                primary_intent = max(intent_scores.items(), key=lambda x: x[1])[0]
        
        return {
            'primary_intent': primary_intent,
            'all_intents': list(intent_scores.keys()),
            'intent_scores': intent_scores,
            'complexity': complexity,
            'word_count': word_count,
            'is_complex': complexity in ['high', 'very_high']
        }

    def select_optimal_model(self, intent_analysis: Dict) -> Dict:
        """ìµœì ì˜ AI ëª¨ë¸ ì„ íƒ ë¡œì§ (ìµœì‹  ëª¨ë¸ ë°˜ì˜)"""
        
        # ê¸°ë³¸ ë§¤í•‘ ì „ëµ
        intent_model_mapping = {
            'complex_reasoning': {
                'primary': 'claude',
                'reason': 'ğŸ§  ë³µì¡í•œ ë…¼ë¦¬/ì¶”ë¡ ì—ëŠ” Claude 3.5 Sonnetì´ ê°€ì¥ ìš°ìˆ˜',
                'backup': 'gemini_advanced'
            },
            'technical': {
                'primary': 'deepseek',  # ì½”ë”©ì€ DeepSeek V3ê°€ ê°•ë ¥í•˜ê³  ì €ë ´
                'reason': 'ğŸ’» ì½”ë“œ ë° ê¸°ìˆ ì  ë¬¸ì œ í•´ê²°ì—ëŠ” DeepSeek V3ê°€ ìµœì í™”',
                'backup': 'claude'
            },
            'mathematical': {
                'primary': 'gemini_advanced', # ë˜ëŠ” DeepSeek
                'reason': 'ğŸ§® ìˆ˜í•™ì /ë…¼ë¦¬ì  ì—°ì‚°ì—ëŠ” Gemini 1.5 Proê°€ ê°•ë ¥',
                'backup': 'deepseek'
            },
            'research': {
                'primary': 'gemini_advanced', # ê¸´ ì»¨í…ìŠ¤íŠ¸ ê°•ì 
                'reason': 'ğŸ“š ë°©ëŒ€í•œ í…ìŠ¤íŠ¸/ì—°êµ¬ ë¶„ì„ì—ëŠ” Geminiì˜ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì°½ì´ ìœ ë¦¬',
                'backup': 'claude'
            },
            'creative': {
                'primary': 'claude',
                'reason': 'ğŸ¨ ìì—°ìŠ¤ëŸ½ê³  ì°½ì˜ì ì¸ ì‘ë¬¸ì€ Claudeê°€ ë›°ì–´ë‚¨',
                'backup': 'gemini'
            },
            'general': {
                'primary': 'gemini',
                'reason': 'âš¡ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì—ëŠ” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ Gemini Flash ì‚¬ìš©',
                'backup': 'claude'
            }
        }
        
        # ë³µì¡ë„ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ê³ ì„±ëŠ¥ ëª¨ë¸ ê°•ì œ ì‚¬ìš©
        if intent_analysis['complexity'] == 'very_high':
            if intent_analysis['primary_intent'] == 'technical':
                 # ë³µì¡í•œ ì½”ë”©ì€ Claudeë‚˜ DeepSeek ìœ ì§€
                 pass
            else:
                # ê·¸ ì™¸ ë³µì¡í•œê±´ Claude ìš°ì„ 
                primary_intent = 'complex_reasoning'
                model_choice = intent_model_mapping['complex_reasoning']
        else:
            model_choice = intent_model_mapping.get(intent_analysis['primary_intent'], intent_model_mapping['general'])
        
        # ëª¨ë¸ ê°€ìš©ì„± ì²´í¬ ë° í´ë°±(Fallback) ë¡œì§
        selected_model = model_choice['primary']
        
        # 1. Claude ì„ íƒ ì‹œ
        if selected_model == 'claude' and not self.openrouter_available:
            selected_model = model_choice['backup']
            model_choice['reason'] += " (Claude í‚¤ ì—†ìŒ -> ë°±ì—… ëª¨ë¸ ì‚¬ìš©)"

        # 2. DeepSeek ì„ íƒ ì‹œ
        if selected_model == 'deepseek' and not self.deepseek_available:
            selected_model = 'gemini' if self.gemini_available else 'backup_unavailable'
            model_choice['reason'] += " (DeepSeek í‚¤ ì—†ìŒ -> Gemini ì‚¬ìš©)"

        # 3. Gemini ì„ íƒ ì‹œ
        if 'gemini' in selected_model and not self.gemini_available:
            selected_model = 'claude' if self.openrouter_available else 'backup_unavailable'
            model_choice['reason'] += " (Gemini í‚¤ ì—†ìŒ -> Claude ì‚¬ìš©)"

        model_choice['primary'] = selected_model
        return model_choice

    def call_advanced_models(self, prompt: str, intent: str, model_type: str) -> Dict:
        """ëª¨ë¸ API í˜¸ì¶œ ì‹¤í–‰"""
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°•í™”
        reasoning_prompts = {
            'complex_reasoning': """ë‹¹ì‹ ì€ ë…¼ë¦¬ì  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¬í˜¸í¡ì„ í•˜ê³  ì°¨ê·¼ì°¨ê·¼ ìƒê°í•´ë³´ì„¸ìš”(Chain of Thought).
            1. í•µì‹¬ ì£¼ì¥ê³¼ ì „ì œë¥¼ ì‹ë³„í•˜ì„¸ìš”.
            2. ë…¼ë¦¬ì  í—ˆì ì´ë‚˜ ëª¨ìˆœì„ ì°¾ìœ¼ì„¸ìš”.
            3. ë‹¤ê°ë„ë¡œ ë¶„ì„í•œ ë’¤ ê²°ë¡ ì„ ë‚´ë¦¬ì„¸ìš”.""",
            
            'technical': """ë‹¹ì‹ ì€ ìˆ˜ì„ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
            1. ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ ë¶„ì„í•˜ì„¸ìš”.
            2. íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            3. ì½”ë“œì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª…ì„ ë§ë¶™ì´ì„¸ìš”.""",
            
            'mathematical': """ë‹¹ì‹ ì€ ìˆ˜í•™ìì…ë‹ˆë‹¤.
            1. ë¬¸ì œë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.
            2. í’€ì´ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ì„¸ìš”.
            3. ìµœì¢… ë‹µì•ˆì„ ê²€ì¦í•˜ì„¸ìš”."""
        }
        
        system_instruction = reasoning_prompts.get(intent, "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ë„ë¡ ë‹µë³€í•˜ì„¸ìš”.")
        full_prompt = f"{system_instruction}\n\nì§ˆë¬¸: {prompt}"
        
        try:
            # --- Google Gemini ---
            if 'gemini' in model_type and self.gemini_available:
                # Gemini ëª¨ë¸ ë²„ì „ ì—…ë°ì´íŠ¸ (1.0 -> 1.5)
                if model_type == 'gemini_advanced':
                    model_name = 'gemini-1.5-pro'
                else:
                    model_name = 'gemini-1.5-flash' # 1.0-pro ëŒ€ì‹  Flash ì‚¬ìš© (ë” ë¹ ë¥´ê³  ì„±ëŠ¥ ì¢‹ìŒ)
                
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(full_prompt)
                
                return {
                    'success': True,
                    'content': response.text,
                    'model': f"Google {model_name}",
                    'tokens': self._estimate_tokens(full_prompt + response.text)
                }
                
            # --- Claude (via OpenRouter) ---
            elif model_type == 'claude' and self.openrouter_available:
                data = {
                    "model": "anthropic/claude-3.5-sonnet",
                    "messages": [{"role": "user", "content": full_prompt}],
                    "max_tokens": 4000, # í† í° ìˆ˜ ì¦ê°€
                    "temperature": 0.2
                }
                
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.openrouter_key}",
                        "HTTP-Referer": "http://localhost:8501", 
                    },
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    tokens = result.get('usage', {}).get('total_tokens', 0)
                    return {'success': True, 'content': content, 'model': 'Claude 3.5 Sonnet', 'tokens': tokens}
                else:
                    return {'success': False, 'error': f'OpenRouter API ì˜¤ë¥˜: {response.status_code} - {response.text}'}
            
            # --- DeepSeek ---
            elif model_type == 'deepseek' and self.deepseek_available:
                data = {
                    "model": "deepseek-chat", # V3 Chat
                    "messages": [{"role": "user", "content": full_prompt}],
                    "max_tokens": 4000,
                    "temperature": 0.1 # ì½”ë”©/ë…¼ë¦¬ëŠ” ë‚®ì€ ì˜¨ë„
                }
                
                response = requests.post(
                    "https://api.deepseek.com/chat/completions",
                    headers={"Authorization": f"Bearer {self.deepseek_key}"},
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    tokens = result.get('usage', {}).get('total_tokens', 0)
                    return {'success': True, 'content': content, 'model': 'DeepSeek V3', 'tokens': tokens}
                else:
                    return {'success': False, 'error': f'DeepSeek API ì˜¤ë¥˜: {response.status_code}'}
            
            return {'success': False, 'error': f'ì„ íƒëœ ëª¨ë¸({model_type})ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.'}
                
        except Exception as e:
            return {'success': False, 'error': f'ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}'}

    def _estimate_tokens(self, text):
        """ê°„ì´ í† í° ê³„ì‚° (Geminiìš©)"""
        return len(text) // 4

# --- UI ì»´í¬ë„ŒíŠ¸ í•¨ìˆ˜ë“¤ ---

def display_sidebar_info(intent_analysis, model_choice):
    """ì‚¬ì´ë“œë°” ì •ë³´ í‘œì‹œ"""
    st.sidebar.header("ğŸ¯ ì˜ë„ ë° ëª¨ë¸ ë¶„ì„")
    
    # ì˜ë„
    st.sidebar.subheader("User Intent")
    primary = intent_analysis['primary_intent']
    st.sidebar.info(f"**ì˜ë„**: {primary.upper()}")
    st.sidebar.text(f"ë³µì¡ë„: {intent_analysis['complexity']}")
    
    # ëª¨ë¸
    st.sidebar.subheader("Selected Model")
    model_name = model_choice['primary']
    st.sidebar.success(f"**ëª¨ë¸**: {model_name}")
    st.sidebar.caption(f"ğŸ’¡ {model_choice['reason']}")
    
    # ëª¨ë¸ë³„ íŠ¹ì§• ì•ˆë‚´
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ† ëª¨ë¸ë³„ íŠ¹ê¸°")
    st.sidebar.markdown("""
    - **Claude 3.5 Sonnet**: 
      ë…¼ë¦¬ì  ì¶”ë¡ , ë‰˜ì•™ìŠ¤ íŒŒì•…, ì‘ë¬¸
    - **Gemini 1.5 Pro/Flash**: 
      ê¸´ ë¬¸ë§¥ ì²˜ë¦¬, ë©€í‹°ëª¨ë‹¬, ì†ë„
    - **DeepSeek V3**: 
      ì½”ë”©, ìˆ˜í•™, ê°€ì„±ë¹„ ìµœê°•
    """)

def main():
    st.title("ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°")
    st.markdown("ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì™€ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ **Claude, Gemini, DeepSeek** ì¤‘ ìµœì ì˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    ai_system = StreamlitAISystem()
    
    # API í‚¤ ìƒíƒœ í™•ì¸ UI
    with st.expander("ğŸ”‘ ì‹œìŠ¤í…œ ìƒíƒœ ë° API í‚¤ í™•ì¸", expanded=False):
        col1, col2, col3 = st.columns(3)
        col1.metric("Gemini", "Ready" if ai_system.gemini_available else "Missing")
        col2.metric("Claude(OpenRouter)", "Ready" if ai_system.openrouter_available else "Missing")
        col3.metric("DeepSeek", "Ready" if ai_system.deepseek_available else "Missing")
        if not any([ai_system.gemini_available, ai_system.openrouter_available, ai_system.deepseek_available]):
            st.error("ì„¤ì •ëœ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. `.streamlit/secrets.toml` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.divider()
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "model" in message:
                st.caption(f"ğŸ›  {message['model']} | ì˜ë„: {message.get('intent', 'N/A')}")

    # ì…ë ¥ì°½
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'íŒŒì´ì¬ìœ¼ë¡œ ë±€ ê²Œì„ ì½”ë“œ ì§œì¤˜', 'ì´ ë…¼ë¦¬ì˜ ëª¨ìˆœì ì€?')"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ë¶„ì„ ë° ìƒì„±
        with st.spinner("ğŸ¤” ì˜ë„ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì„ ì • ì¤‘..."):
            # 1. ì˜ë„ ë¶„ì„
            intent_analysis = ai_system.advanced_intent_analysis(prompt)
            # 2. ëª¨ë¸ ì„ íƒ
            model_choice = ai_system.select_optimal_model(intent_analysis)
            
            # ì‚¬ì´ë“œë°” ì—…ë°ì´íŠ¸
            display_sidebar_info(intent_analysis, model_choice)
            
        with st.chat_message("assistant"):
            msg_placeholder = st.empty()
            
            # 3. ëª¨ë¸ í˜¸ì¶œ
            with st.spinner(f"ğŸš€ {model_choice['primary']} ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = ai_system.call_advanced_models(
                    prompt, 
                    intent_analysis['primary_intent'],
                    model_choice['primary']
                )
            
            if response['success']:
                msg_placeholder.markdown(response['content'])
                
                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                st.success(f"Used: **{response['model']}** (Tokens: approx. {response['tokens']})")
                
                # ê¸°ë¡ ì €ì¥
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response['content'],
                    "model": response['model'],
                    "intent": intent_analysis['primary_intent']
                })
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                if response['model'] not in st.session_state.model_usage:
                    st.session_state.model_usage[response['model']] = 0
                st.session_state.model_usage[response['model']] += 1
                
            else:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {response.get('error')}")
                st.info("ì œì•ˆ: ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ì§ˆë¬¸ì„ ë³€ê²½í•˜ê±°ë‚˜ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
