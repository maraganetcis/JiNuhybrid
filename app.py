import os
import asyncio
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum
import google.generativeai as genai
import requests
from openai import OpenAI
from anthropic import Anthropic
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelProvider(Enum):
    GOOGLE = "google"
    OPENROUTER = "openrouter"
    ANTHROPIC = "anthropic"
    DEEPSEEK = "deepseek"
    OPENAI = "openai"

@dataclass
class ModelConfig:
    provider: ModelProvider
    model_name: str
    api_key: str
    base_url: Optional[str] = None
    cost_per_input: float = 0.0
    cost_per_output: float = 0.0

class HybridAISystem:
    def __init__(self):
        # API í‚¤ ì´ˆê¸°í™”
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        self.openrouter_key = os.getenv('OPENROUTER_API_KEY')
        self.anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        self.deepseek_key = os.getenv('DEEPSEEK_API_KEY')
        
        # ëª¨ë¸ êµ¬ì„±
        self.models = {
            'gemini_flash': ModelConfig(
                provider=ModelProvider.GOOGLE,
                model_name='gemini-2.0-flash',
                api_key=self.google_api_key,
                cost_per_input=0.075,  # $0.75 per 1M tokens
                cost_per_output=0.30   # $3.00 per 1M tokens
            ),
            'gemini_pro': ModelConfig(
                provider=ModelProvider.GOOGLE,
                model_name='gemini-1.5-pro',
                api_key=self.google_api_key,
                cost_per_input=3.75,   # $7.5 per 1M tokens
                cost_per_output=15.00  # $15.0 per 1M tokens
            ),
            'claude_sonnet': ModelConfig(
                provider=ModelProvider.OPENROUTER,
                model_name='anthropic/claude-3.5-sonnet',
                api_key=self.openrouter_key,
                base_url='https://openrouter.ai/api/v1',
                cost_per_input=3.0,    # $3.0 per 1M tokens
                cost_per_output=15.0   # $15.0 per 1M tokens
            ),
            'deepseek_v3': ModelConfig(
                provider=ModelProvider.DEEPSEEK,
                model_name='deepseek-chat',
                api_key=self.deepseek_key,
                base_url='https://api.deepseek.com/v1',
                cost_per_input=0.14,   # $1.4 per 1M tokens
                cost_per_output=0.28   # $2.8 per 1M tokens
            ),
            'llama_70b': ModelConfig(
                provider=ModelProvider.OPENROUTER,
                model_name='meta-llama/llama-3-70b-instruct',
                api_key=self.openrouter_key,
                base_url='https://openrouter.ai/api/v1',
                cost_per_input=0.59,   # $0.59 per 1M tokens
                cost_per_output=0.79   # $0.79 per 1M tokens
            )
        }
        
        # ì œê³µìë³„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.google_api_key:
            genai.configure(api_key=self.google_api_key)
        
        self.openai_client = OpenAI(api_key=self.deepseek_key) if self.deepseek_key else None
        self.anthropic_client = Anthropic(api_key=self.anthropic_key) if self.anthropic_key else None
        
        self.available_models = self._check_available_models()
    
    def _check_available_models(self) -> Dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸"""
        available = {}
        
        # Google ëª¨ë¸ í™•ì¸
        if self.google_api_key:
            available['gemini_flash'] = self.models['gemini_flash']
            available['gemini_pro'] = self.models['gemini_pro']
        
        # OpenRouter ëª¨ë¸ í™•ì¸
        if self.openrouter_key:
            available['claude_sonnet'] = self.models['claude_sonnet']
            available['llama_70b'] = self.models['llama_70b']
        
        # DeepSeek ëª¨ë¸ í™•ì¸
        if self.deepseek_key:
            available['deepseek_v3'] = self.models['deepseek_v3']
        
        logger.info(f"Available models: {list(available.keys())}")
        return available

    def advanced_intent_analysis(self, user_input: str) -> Dict:
        """ê³ ê¸‰ ì˜ë„ ë¶„ì„ ì‹œìŠ¤í…œ - ë³µì¡í•œ ì¶”ë¡  ì¶”ê°€"""
        intent_keywords = {
            'complex_reasoning': [
                'ë…¼ë¦¬', 'ì¶”ë¡ ', 'ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'íŒë‹¨', 'ê²°ë¡ ', 'ê°€ì •',
                'ì „ì œ', 'ë…¼ì¦', 'íƒ€ë‹¹ì„±', 'ë¹„íŒì ', 'ì‚¬ê³ ', 'ì´ìœ ', 'ê·¼ê±°',
                'ë³µì¡í•œ', 'ë‚œì´ë„', 'ì‹¬ì¸µ', 'ë‹¤ë‹¨ê³„', 'ì¢…í•©', 'í†µí•©', 'ë…¼ë¬¸',
                'ì—°êµ¬', 'ì‹¤í—˜', 'ê°€ì„¤', 'ê²€ì¦'
            ],
            'technical': [
                'ì½”ë“œ', 'í”„ë¡œê·¸ë˜ë°', 'ì•Œê³ ë¦¬ì¦˜', 'ê°œë°œ', 'ì„¤ê³„', 'íŒŒì´ì¬', 
                'ìë°”', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'ë¦¬ì•¡íŠ¸', 'vue', 'html', 'css',
                'ë””ë²„ê·¸', 'ë²„ê·¸', 'ì˜¤ë¥˜', 'ì»´íŒŒì¼', 'í•¨ìˆ˜', 'í´ë˜ìŠ¤'
            ],
            'creative': [
                'ì‘ì„±', 'ìƒì„±', 'ë§Œë“¤', 'ê¸€ì“°ê¸°', 'ì‹œ', 'ì´ì•¼ê¸°', 'ì°½ì˜',
                'ì•„ì´ë””ì–´', 'ê¸°íš', 'ì½˜í…ì¸ ', 'ë§ˆì¼€íŒ…', 'ê´‘ê³ ', 'ë¸Œëœë“œ'
            ],
            'mathematical': [
                'ê³„ì‚°', 'ìˆ˜í•™', 'ê³µì‹', 'ë°©ì •ì‹', 'í†µê³„', 'í™•ë¥ ', 'ë¯¸ë¶„',
                'ì ë¶„', 'í•¨ìˆ˜', 'ê¸°í•˜', 'ëŒ€ìˆ˜', 'ì‚¼ê°í•¨ìˆ˜', 'í–‰ë ¬'
            ],
            'research': [
                'ì—°êµ¬', 'ë…¼ë¬¸', 'ì°¸ê³ ë¬¸í—Œ', 'í•™ìˆ ', 'ì´ë¡ ', 'ì‹¤í—˜', 'ë°ì´í„°',
                'ë¶„ì„', 'í†µê³„', 'ì„¤ë¬¸', 'ì¡°ì‚¬', 'ë¦¬ì„œì¹˜'
            ],
            'factual': [
                'ë­ì•¼', 'ë¬´ì—‡', 'ì•Œë ¤ì¤˜', 'ì •ë³´', 'ì‚¬ì‹¤', 'ì •ì˜', 'ì˜ë¯¸',
                'ê°œë…', 'ì›ë¦¬', 'ë°©ë²•'
            ],
            'casual': [
                'ì•ˆë…•', 'í•˜ì´', 'ì˜ì§€ë‚´', 'ê³ ë§ˆì›Œ', 'ë°˜ê°€ì›Œ', 'ã…ã…', 'ã…‹ã…‹'
            ]
        }
        
        # ì˜ë„ ì ìˆ˜ ê³„ì‚°
        intent_scores = {}
        user_lower = user_input.lower()
        
        for intent, keywords in intent_keywords.items():
            score = sum(10 for keyword in keywords if keyword in user_lower)
            if score > 0:
                intent_scores[intent] = score
        
        # ë³µì¡ë„ ë¶„ì„ ê°•í™”
        word_count = len(user_input.split())
        sentence_count = user_input.count('.') + user_input.count('?') + user_input.count('!')
        
        has_complex_indicators = any(word in user_lower for word in [
            'ë¶„ì„', 'ë¹„êµ', 'í‰ê°€', 'ë…¼ë¦¬', 'ì¶”ë¡ ', 'ì „ì œ', 'ê²°ë¡ ', 'ì—°êµ¬'
        ])
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity_score = word_count * 0.5 + sentence_count * 2
        
        if complexity_score > 25 or has_complex_indicators:
            complexity = 'very_high'
        elif complexity_score > 15:
            complexity = 'high'
        elif complexity_score > 8:
            complexity = 'medium'
        else:
            complexity = 'low'
        
        # ì£¼ìš” ì˜ë„ ì„ íƒ (ë³µì¡í•œ ì¶”ë¡  ìš°ì„ )
        if 'complex_reasoning' in intent_scores:
            primary_intent = 'complex_reasoning'
        elif intent_scores:
            primary_intent = max(intent_scores, key=intent_scores.get())
        else:
            primary_intent = 'general'
        
        return {
            'primary_intent': primary_intent,
            'all_intents': list(intent_scores.keys()),
            'intent_scores': intent_scores,
            'complexity': complexity,
            'word_count': word_count,
            'sentence_count': sentence_count,
            'complexity_score': complexity_score,
            'is_complex': complexity in ['high', 'very_high']
        }

    def select_optimal_model(self, intent_analysis: Dict, budget_conscious: bool = True) -> Dict:
        """ìµœì ì˜ AI ëª¨ë¸ ì„ íƒ - ë¹„ìš© íš¨ìœ¨ì„± ê³ ë ¤"""
        
        # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ë§¤í•‘
        cost_effective_mapping = {
            'complex_reasoning': {
                'primary': 'claude_sonnet',
                'reason': 'ğŸ§  ë³µì¡í•œ ì¶”ë¡ ì—ëŠ” Claude 3.5 Sonnetì´ ê°€ì¥ ìš°ìˆ˜í•¨',
                'backup': 'gemini_pro'
            },
            'technical': {
                'primary': 'gemini_flash',
                'reason': 'ğŸ”§ ê¸°ìˆ /ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” Gemini Flashê°€ ë¹ ë¥´ê³  ì •í™•í•¨',
                'backup': 'deepseek_v3'
            },
            'mathematical': {
                'primary': 'gemini_flash',
                'reason': 'ğŸ§® ìˆ˜í•™ì  ë¬¸ì œì—ëŠ” Gemini Flashì˜ ì •í™•ë„ê°€ ë†’ìŒ',
                'backup': 'deepseek_v3'
            },
            'research': {
                'primary': 'claude_sonnet',
                'reason': 'ğŸ“Š ì—°êµ¬/í•™ìˆ  ë¶„ì„ì—ëŠ” Claudeì˜ ê¹Šì€ ì´í•´ë ¥ì´ ì í•©',
                'backup': 'gemini_pro'
            },
            'creative': {
                'primary': 'claude_sonnet',
                'reason': 'ğŸ¨ ì°½ì˜ì  ì‘ì—…ì—ëŠ” Claudeì˜ ìœ ì—°ì„±ì´ ì¢‹ìŒ',
                'backup': 'gemini_pro'
            },
            'general': {
                'primary': 'gemini_flash',
                'reason': 'âš¡ ì¼ë°˜ ì§ˆë¬¸ì—ëŠ” Gemini Flashì˜ ë¹ ë¥¸ ì‘ë‹µì´ ì í•©',
                'backup': 'deepseek_v3'
            },
            'factual': {
                'primary': 'deepseek_v3',
                'reason': 'ğŸ’° ì‚¬ì‹¤ í™•ì¸ì—ëŠ” ê°€ì¥ ì €ë ´í•œ DeepSeekì´ íš¨ìœ¨ì ',
                'backup': 'gemini_flash'
            }
        }
        
        # ê³ ì„±ëŠ¥ ëª¨ë¸ ë§¤í•‘ (ë¹„ìš© ëœ ì¤‘ìš”)
        performance_mapping = {
            'complex_reasoning': {
                'primary': 'claude_sonnet',
                'reason': 'ğŸ§  ìµœê³  ìˆ˜ì¤€ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ìœ„í•œ Claude 3.5 Sonnet',
                'backup': 'gemini_pro'
            },
            'technical': {
                'primary': 'gemini_pro',
                'reason': 'ğŸ”§ ì •ë°€í•œ ê¸°ìˆ  ì‘ì—…ì—ëŠ” Gemini Proê°€ ì í•©',
                'backup': 'claude_sonnet'
            },
            # ... ë‚˜ë¨¸ì§€ ì˜ë„ë“¤ë„ ìœ ì‚¬í•˜ê²Œ êµ¬ì„±
        }
        
        # ë§¤í•‘ ì„ íƒ
        model_mapping = cost_effective_mapping if budget_conscious else performance_mapping
        
        # ë³µì¡ë„ê°€ ë§¤ìš° ë†’ìœ¼ë©´ ë³µì¡í•œ ì¶”ë¡  ëª¨ë¸ ê°•ì œ ì‚¬ìš©
        if intent_analysis['complexity'] == 'very_high':
            primary_intent = 'complex_reasoning'
        else:
            primary_intent = intent_analysis['primary_intent']
        
        model_choice = model_mapping.get(primary_intent, model_mapping['general'])
        
        # ì„ íƒëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if model_choice['primary'] not in self.available_models:
            model_choice['primary'] = model_choice['backup']
        
        return model_choice

    async def call_model(self, prompt: str, model_config: ModelConfig, intent: str) -> Dict:
        """ëª¨ë¸ í˜¸ì¶œ - ë¹„ë™ê¸° ì²˜ë¦¬"""
        
        reasoning_prompts = {
            'complex_reasoning': """
            ë‹¹ì‹ ì€ ë…¼ë¦¬ì  ì¶”ë¡  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì ‘ê·¼í•´ì£¼ì„¸ìš”:
            
            1. **ë¬¸ì œ ë¶„ì„**: í•µì‹¬ ìš”ì†Œì™€ ì£¼ìš” ê°œë… íŒŒì•…
            2. **ì „ì œ í™•ì¸**: ëª…ì‹œì /ì•”ë¬µì  ê°€ì • ì‹ë³„
            3. **ë…¼ë¦¬ êµ¬ì¡°**: ì£¼ì¥ê³¼ ê·¼ê±°ì˜ ì—°ê²° ê´€ê³„ ë¶„ì„
            4. **ë¹„íŒì  ê²€í† **: íƒ€ë‹¹ì„±ê³¼ í•œê³„ì  í‰ê°€
            5. **ê²°ë¡  ë„ì¶œ**: ì²´ê³„ì ì¸ ì¶”ë¡ ì„ í†µí•œ ìµœì¢… íŒë‹¨
            
            ì§ˆë¬¸: {prompt}
            """,
            'technical': """
            ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:
            
            1. **ìš”êµ¬ì‚¬í•­ ë¶„ì„**: ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ ëª…í™•íˆ ì´í•´
            2. **ì•„í‚¤í…ì²˜ ì„¤ê³„**: ìµœì ì˜ ì†”ë£¨ì…˜ êµ¬ì¡° ì œì•ˆ
            3. **ì½”ë“œ êµ¬í˜„**: ì‹¤ìš©ì ì´ê³  íš¨ìœ¨ì ì¸ ì½”ë“œ ì‘ì„±
            4. **í…ŒìŠ¤íŠ¸ ê³„íš**: ê²€ì¦ ê°€ëŠ¥í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì œì‹œ
            5. **ì„±ëŠ¥ ê³ ë ¤**: í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„± ê³ ë ¤
            
            ì§ˆë¬¸: {prompt}
            """,
            'mathematical': """
            ë‹¹ì‹ ì€ ìˆ˜í•™ì  ë¬¸ì œ í•´ê²° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼í•´ì£¼ì„¸ìš”:
            
            1. **ë¬¸ì œ ì´í•´**: ì£¼ì–´ì§„ ì¡°ê±´ê³¼ êµ¬í•´ì•¼ í•˜ëŠ” ê°’ ì •ì˜
            2. **ì ‘ê·¼ë²• ì„ íƒ**: ì ì ˆí•œ ê³µì‹/ì´ë¡ /ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
            3. **ë‹¨ê³„ì  ê³„ì‚°**: ì²´ê³„ì ì¸ ê³„ì‚° ê³¼ì • ì œì‹œ
            4. **ê²°ê³¼ ê²€ì¦**: ë‹µë³€ì˜ íƒ€ë‹¹ì„± í™•ì¸
            5. **ëŒ€ì•ˆ ì œì‹œ**: ë‹¤ë¥¸ ì ‘ê·¼ë²• ê°€ëŠ¥ì„± íƒìƒ‰
            
            ì§ˆë¬¸: {prompt}
            """
        }
        
        specialized_prompt = reasoning_prompts.get(
            intent, 
            "ëª…í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: {prompt}"
        ).format(prompt=prompt)
        
        try:
            if model_config.provider == ModelProvider.GOOGLE:
                return await self._call_google_model(specialized_prompt, model_config)
            elif model_config.provider == ModelProvider.OPENROUTER:
                return await self._call_openrouter_model(specialized_prompt, model_config)
            elif model_config.provider == ModelProvider.DEEPSEEK:
                return await self._call_deepseek_model(specialized_prompt, model_config)
            else:
                raise ValueError(f"Unsupported provider: {model_config.provider}")
                
        except Exception as e:
            logger.error(f"Model call failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'content': None,
                'tokens_used': 0,
                'cost': 0.0
            }

    async def _call_google_model(self, prompt: str, config: ModelConfig) -> Dict:
        """Google Gemini ëª¨ë¸ í˜¸ì¶œ"""
        model = genai.GenerativeModel(config.model_name)
        response = model.generate_content(prompt)
        
        return {
            'success': True,
            'content': response.text,
            'tokens_used': len(prompt.split()) + len(response.text.split()),  # ì¶”ì •ì¹˜
            'cost': 0.0,  # ì‹¤ì œë¡œëŠ” ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚° í•„ìš”
            'model': config.model_name
        }

    async def _call_openrouter_model(self, prompt: str, config: ModelConfig) -> Dict:
        """OpenRouter ëª¨ë¸ í˜¸ì¶œ"""
        data = {
            "model": config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        response = requests.post(
            f"{config.base_url}/chat/completions",
            headers={"Authorization": f"Bearer {config.api_key}"},
            json=data,
            timeout=45
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            tokens_used = result.get('usage', {}).get('total_tokens', 0)
            
            return {
                'success': True,
                'content': content,
                'tokens_used': tokens_used,
                'cost': (tokens_used / 1000000) * config.cost_per_input,  # ë‹¨ìˆœí™”
                'model': config.model_name
            }
        else:
            raise Exception(f"OpenRouter API error: {response.status_code}")

    async def _call_deepseek_model(self, prompt: str, config: ModelConfig) -> Dict:
        """DeepSeek ëª¨ë¸ í˜¸ì¶œ"""
        data = {
            "model": config.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        response = requests.post(
            f"{config.base_url}/chat/completions",
            headers={"Authorization": f"Bearer {config.api_key}"},
            json=data,
            timeout=45
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            tokens_used = result.get('usage', {}).get('total_tokens', 0)
            
            return {
                'success': True,
                'content': content,
                'tokens_used': tokens_used,
                'cost': (tokens_used / 1000000) * config.cost_per_input,
                'model': config.model_name
            }
        else:
            raise Exception(f"DeepSeek API error: {response.status_code}")

    async def process_query(self, user_input: str, budget_conscious: bool = True) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        
        # 1. ì˜ë„ ë¶„ì„
        intent_analysis = self.advanced_intent_analysis(user_input)
        logger.info(f"Intent analysis: {intent_analysis}")
        
        # 2. ëª¨ë¸ ì„ íƒ
        model_choice = self.select_optimal_model(intent_analysis, budget_conscious)
        logger.info(f"Model choice: {model_choice}")
        
        # 3. ëª¨ë¸ í˜¸ì¶œ
        model_config = self.available_models[model_choice['primary']]
        response = await self.call_model(
            user_input, 
            model_config, 
            intent_analysis['primary_intent']
        )
        
        return {
            'intent_analysis': intent_analysis,
            'model_choice': model_choice,
            'response': response,
            'timestamp': asyncio.get_event_loop().time()
        }

# ì‚¬ìš© ì˜ˆì œ
async def main():
    system = HybridAISystem()
    
    test_queries = [
        "íŒŒì´ì¬ì—ì„œ ë‹¤ì¤‘ ìƒì†ì˜ ì¥ë‹¨ì ê³¼ MRO(Method Resolution Order)ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
        "ê¸°í›„ ë³€í™”ê°€ ê²½ì œ ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜",
        "ì•ˆë…•! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë•Œ?",
        "ë¯¸ë¶„ë°©ì •ì‹ê³¼ ì„ í˜•ëŒ€ìˆ˜ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"Query: {query}")
        print(f"{'='*50}")
        
        result = await system.process_query(query, budget_conscious=True)
        
        if result['response']['success']:
            print(f"Intent: {result['intent_analysis']['primary_intent']}")
            print(f"Model: {result['model_choice']['primary']}")
            print(f"Reason: {result['model_choice']['reason']}")
            print(f"Response: {result['response']['content'][:200]}...")
            print(f"Tokens used: {result['response']['tokens_used']}")
            print(f"Estimated cost: ${result['response']['cost']:.6f}")
        else:
            print(f"Error: {result['response']['error']}")

if __name__ == "__main__":
    asyncio.run(main())
